{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Multi-HMC Gibbs\n",
    "\n",
    "This notebook will run through some examples using the new Multi-HMC Gibbs sampler.\n",
    "\n",
    "First import sme packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import arviz\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpyro.infer import MCMC, NUTS, HMCGibbs\n",
    "from jax import random\n",
    "\n",
    "from MultiHMCGibbs import MultiHMCGibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My `custom_gibbs` file is one folder up, so add it to the python path before import (if in the same folder just keep the last line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D normal distribution\n",
    "\n",
    "This can be sampled just fine with HMC, and has an analytic Gibbs step, so `NUTS`, `HMCGibbs`, and `MultiHMCGibbs` can all be tested head to head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    x = numpyro.sample(\"x\", dist.Normal(0.0, 2.0))\n",
    "    y = numpyro.sample(\"y\", dist.Normal(0.0, 2.0))\n",
    "    numpyro.sample(\"obs\", dist.Normal(x + y, 1.0), obs=jnp.array([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_kernel = NUTS(model)\n",
    "mcmc = MCMC(\n",
    "    hmc_kernel,\n",
    "    num_warmup=1000,\n",
    "    num_samples=5000,\n",
    "    num_chains=4,\n",
    "    chain_method='vectorized',\n",
    "    progress_bar=False\n",
    ")\n",
    "mcmc.run(random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data_hmc = arviz.from_numpyro(mcmc)\n",
    "print(f'divergences per chain: {inf_data_hmc.sample_stats.diverging.values.sum(axis=1)}')\n",
    "display(arviz.summary(inf_data_hmc))\n",
    "fig = corner.corner(inf_data_hmc, color='C0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHMCGibbs\n",
    "\n",
    "To use the new `MutliHMCGibbs` you need to create a list of HMC kernels (`NUTS` in this case, each can have their own keywords such as `target_accept_prob` or `max_tree_depth`).  The other argument is a list of lists containing the **free** parameters for each of the inner kernels.\n",
    "\n",
    "**Important**: All free parameters must be listed **exactly once** for the sampler to work.  I have not implemented any checks for this yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_kernels = [\n",
    "    NUTS(model),\n",
    "    NUTS(model)\n",
    "]\n",
    "\n",
    "outer_kernel = MultiHMCGibbs(\n",
    "    inner_kernels,\n",
    "    [['y'], ['x']]\n",
    ")\n",
    "\n",
    "mcmc_gibbs = MCMC(\n",
    "    outer_kernel,\n",
    "    num_warmup=1000,\n",
    "    num_samples=5000,\n",
    "    num_chains=4,\n",
    "    chain_method='vectorized',\n",
    "    progress_bar=False\n",
    ")\n",
    "mcmc_gibbs.run(random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data_gibbs = arviz.from_numpyro(mcmc_gibbs)\n",
    "print('HMC (Blue)')\n",
    "print('MultiHMCGibbs (Orange)')\n",
    "print(f'divergences per chain per step:\\n {inf_data_gibbs.sample_stats.diverging.values.sum(axis=1).T}')\n",
    "display(arviz.summary(inf_data_gibbs))\n",
    "\n",
    "fig = corner.corner(inf_data_gibbs, color='C1')\n",
    "_ = corner.corner(inf_data_hmc, fig=fig, color='C0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMCGibbs\n",
    "\n",
    "This distribution has an analytic Gibbs step so it can use the built in `HMCGibbs`, let's try that and compare.  We need to use `sequential` as `vectorized` is currently broken for `HMCGibbs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_fn(rng_key, gibbs_sites, hmc_sites):\n",
    "    y = hmc_sites['y']\n",
    "    new_x = dist.Normal(0.8 * (1-y), jnp.sqrt(0.8)).sample(rng_key)\n",
    "    return {'x': new_x}\n",
    "\n",
    "\n",
    "kernel_gibbs_fn = HMCGibbs(hmc_kernel, gibbs_fn=gibbs_fn, gibbs_sites=['x'])\n",
    "\n",
    "mcmc_gibbs_fn = MCMC(\n",
    "    kernel_gibbs_fn,\n",
    "    num_warmup=1000,\n",
    "    num_samples=5000,\n",
    "    num_chains=4,\n",
    "    chain_method='sequential',\n",
    "    progress_bar=False\n",
    ")\n",
    "\n",
    "mcmc_gibbs_fn.run(random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data_gibbs_fn = arviz.from_numpyro(mcmc_gibbs_fn)\n",
    "print('HMC (Blue)')\n",
    "print('MultiHMCGibbs (Orange)')\n",
    "print('HMCGibbs (Green)')\n",
    "display(arviz.summary(inf_data_gibbs_fn))\n",
    "\n",
    "fig = corner.corner(inf_data_gibbs, color='C1')\n",
    "_ = corner.corner(inf_data_hmc, fig=fig, color='C0')\n",
    "_ = corner.corner(inf_data_gibbs_fn, fig=fig, color='C2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all three cases we got the same results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neal's Funnel\n",
    "\n",
    "Now lets take a distribution where a Gibbs step is needed to get a decent result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dim=10):\n",
    "    y = numpyro.sample(\"y\", dist.Normal(0, 3))\n",
    "    numpyro.sample(\"x\", dist.Normal(jnp.zeros(dim - 1), jnp.exp(y / 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(kernel, chain_method, rng_key):\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=8000,\n",
    "        num_samples=5000,\n",
    "        num_chains=4,\n",
    "        chain_method=chain_method,\n",
    "        progress_bar=False\n",
    "    )\n",
    "    mcmc.run(rng_key)\n",
    "    return mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUTS\n",
    "\n",
    "We will use a large `target_accept_prob` to get rid of most divergent samples and use a large number of warmup and samples to get the `r_hat`s down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_mcmc_hmc = run_inference(NUTS(model, target_accept_prob=0.995), 'vectorized', random.PRNGKey(0))\n",
    "inf_funnle_hmc = arviz.from_numpyro(funnel_mcmc_hmc)\n",
    "print(f'divergences per chain: {inf_funnle_hmc.sample_stats.diverging.values.sum(axis=1)}')\n",
    "display(arviz.summary(inf_funnle_hmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_marginal_true = jnp.linspace(-10, 10, 1000)\n",
    "y_marginal_true = jnp.exp(dist.Normal(0, 3).log_prob(x_marginal_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model_hmc = inf_funnle_hmc.posterior.x[..., 0].data.flatten()\n",
    "y_model_hmc = inf_funnle_hmc.posterior.y.data.flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(x_model_hmc, y_model_hmc, '.')\n",
    "plt.xlabel('x[0]')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-100, 100)\n",
    "plt.subplot(122)\n",
    "plt.hist(y_model_hmc, bins=30, histtype='step', density=True, label='HMC')\n",
    "plt.plot(x_marginal_true, y_marginal_true, color='k', label='True marginal')\n",
    "plt.xlabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `NUTS` is struggling with this model.  We can see that the `y` marginal is still missing a bit of negative values at the bottom of the funnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHMCGibbs\n",
    "\n",
    "For the `MultiHMCGibbs` sampler we will only put a large `target_accept_prob` on the `x` values (as these are the difficult ones to draw), but keep the default value for the `y` values.  To keep it on the same footing as the previous run we will use the same number of warm up and sample draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_mcmc_gibbs = run_inference(\n",
    "    MultiHMCGibbs(\n",
    "        [NUTS(model, target_accept_prob=0.995), NUTS(model, target_accept_prob=0.8)],\n",
    "        [['x'], ['y']]\n",
    "    ),\n",
    "    'vectorized',\n",
    "    random.PRNGKey(0)\n",
    ")\n",
    "inf_funnle_gibbs = arviz.from_numpyro(funnel_mcmc_gibbs)\n",
    "print(f'divergences per chain per step:\\n {inf_funnle_gibbs.sample_stats.diverging.values.sum(axis=1).T}')\n",
    "display(arviz.summary(inf_funnle_gibbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model_gibbs = inf_funnle_gibbs.posterior.x[..., 0].data.flatten()\n",
    "y_model_gibbs = inf_funnle_gibbs.posterior.y.data.flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(x_model_hmc, y_model_hmc, '.', label='HMC', zorder=2)\n",
    "plt.plot(x_model_gibbs, y_model_gibbs, '.', label='Gibbs', zorder=1)\n",
    "plt.xlabel('x[0]')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.xlim(-100, 100)\n",
    "plt.subplot(122)\n",
    "plt.hist(y_model_hmc, bins=30, histtype='step', label='HMC', density=True)\n",
    "plt.hist(y_model_gibbs, bins=30, histtype='step', label='Gibbs', density=True)\n",
    "plt.plot(x_marginal_true, y_marginal_true, color='k', label='True marginal')\n",
    "plt.xlabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with the same set up `MultiHMCGibbs` was able to reach deeper into the funnel and pull out the negative `y` values missed by `NUTS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other notes\n",
    "\n",
    "- You can use as many `inner_kernels` as you want\n",
    "- The order the kernels are stepped in is set by the order of the parameter list (in the example above `x` septs first, followed by `y`)\n",
    "- The order matters!  Typically you want to step the parameters closest to the likelihood first and the hyper-parameters second.  But for some models this might not be so clear, so some experimentation could be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
